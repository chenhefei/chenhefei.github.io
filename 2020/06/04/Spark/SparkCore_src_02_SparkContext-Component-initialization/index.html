<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="https://gitee.com/hoffeechen/image/raw/master/img/20200530215328.png?v=2.0.1" type="image/png" sizes="16x16"><link rel="icon" href="https://gitee.com/hoffeechen/image/raw/master/img/20200530215330.png?v=2.0.1" type="image/png" sizes="32x32"><meta name="description" content="转载 Spark Core 源码分析 02">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Core - 源码分析_02 - SparkContext组件初始化">
<meta property="og:url" content="https://chenhefei.github.io/2020/06/04/Spark/SparkCore_src_02_SparkContext-Component-initialization/index.html">
<meta property="og:site_name" content="随机漫步的傻瓜">
<meta property="og:description" content="转载 Spark Core 源码分析 02">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lormer.cn/sq2uE6cicHYyGduHdniaiczqQs6dj6WOdts4nzicrvFQLqCe9ET5ml3gWBMjP7s4kiaxq8AKjM5U5uwVHMNN5VmAFgg.jpg">
<meta property="article:published_time" content="2020-06-04T05:11:15.000Z">
<meta property="article:modified_time" content="2020-06-04T05:38:02.537Z">
<meta property="article:author" content="Paradiser">
<meta property="article:tag" content="大数据框架">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lormer.cn/sq2uE6cicHYyGduHdniaiczqQs6dj6WOdts4nzicrvFQLqCe9ET5ml3gWBMjP7s4kiaxq8AKjM5U5uwVHMNN5VmAFgg.jpg"><meta name="keywords" content="Paradiser, 随机漫步的傻瓜"><meta name="description" content=""><title>Spark Core - 源码分析_02 - SparkContext组件初始化 | 随机漫步的傻瓜</title><link ref="canonical" href="https://chenhefei.github.io/2020/06/04/Spark/SparkCore_src_02_SparkContext-Component-initialization/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.0.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"10px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":true},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":true},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">随机漫步的傻瓜</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Spark Core - 源码分析_02 - SparkContext组件初始化</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-06-04</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2020-06-04</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">3.2k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">24分</span></span></div></header><div class="post-body">
        <h2 id="Spark-Core源码精读-02-SparkContext组件初始化"   >
          <a href="#Spark-Core源码精读-02-SparkContext组件初始化" class="heading-link"><i class="fas fa-link"></i></a>Spark Core源码精读_02 | SparkContext组件初始化</h2>
      <p><strong>目录</strong></p>
<ul>
<li>前言</li>
<li>SparkContext类的构造方法</li>
<li>SparkContext初始化的组件</li>
<li><ul>
<li>SparkConf</li>
<li>LiveListenerBus</li>
<li>AppStatusStore</li>
<li>SparkEnv</li>
<li>SparkStatusTracker</li>
<li>ConsoleProgressBar</li>
<li>SparkUI</li>
<li>(Hadoop)Configuration</li>
<li>HeartbeatReceiver</li>
<li>SchedulerBackend</li>
<li>TaskScheduler</li>
<li>DAGScheduler</li>
<li>EventLoggingListener</li>
<li>ExecutorAllocationManager</li>
<li>ContextCleaner</li>
</ul>
</li>
<li>总结</li>
</ul>
<hr>

        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>SparkContext在整个Spark Core中的地位毋庸置疑，可以说是核心中的核心。它存在于Driver中，是Spark功能的主要入口，如果没有SparkContext，我们的应用就无法运行，也就无从享受Spark为我们带来的种种便利。</p>
<p>由于SparkContext类的内容较多（整个SparkContext.scala文件共有2900多行），因此我们不追求毕其功于一役，而是拆成三篇文章来讨论。本文主要研究SparkContext初始化过程中涉及到的那些Spark组件，并对它们进行介绍。</p>

        <h3 id="SparkContext类的构造方法"   >
          <a href="#SparkContext类的构造方法" class="heading-link"><i class="fas fa-link"></i></a>SparkContext类的构造方法</h3>
      <p>SparkContext类接收SparkConf作为构造参数，并且有多种辅助构造方法的实现，比较简单，不多废话了。</p>
<p><strong>代码2.1 - o.a.s.SparkContext类的辅助构造方法</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkContext</span>(<span class="params">config: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() = <span class="keyword">this</span>(<span class="keyword">new</span> <span class="type">SparkConf</span>())</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>, conf: <span class="type">SparkConf</span>) =</span><br><span class="line"><span class="keyword">this</span>(<span class="type">SparkContext</span>.updatedConf(conf, master, appName))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(</span><br><span class="line">      master: <span class="type">String</span>,</span><br><span class="line">      appName: <span class="type">String</span>,</span><br><span class="line">      sparkHome: <span class="type">String</span> = <span class="literal">null</span>,</span><br><span class="line">      jars: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span>,</span><br><span class="line">      environment: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>()) = &#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="type">SparkContext</span>.updatedConf(<span class="keyword">new</span> <span class="type">SparkConf</span>(), master, appName, sparkHome, jars, environment))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>) =</span><br><span class="line"><span class="keyword">this</span>(master, appName, <span class="literal">null</span>, <span class="type">Nil</span>, <span class="type">Map</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>, sparkHome: <span class="type">String</span>) =</span><br><span class="line"><span class="keyword">this</span>(master, appName, sparkHome, <span class="type">Nil</span>, <span class="type">Map</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>, sparkHome: <span class="type">String</span>, jars: <span class="type">Seq</span>[<span class="type">String</span>]) =</span><br><span class="line"><span class="keyword">this</span>(master, appName, sparkHome, jars, <span class="type">Map</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>而其主构造方法主要由一个巨大的try-catch块组成，位于SparkContext.scala的362~586行，它内部包含了很多初始化逻辑。</p>

        <h3 id="SparkContext初始化的组件"   >
          <a href="#SparkContext初始化的组件" class="heading-link"><i class="fas fa-link"></i></a>SparkContext初始化的组件</h3>
      <p>在上述try-catch块的前方，有一批预先声明的私有变量字段，且基本都重新定义了对应的Getter方法。它们用于维护SparkContext需要初始化的所有组件的内部状态。下面的代码清单将它们预先整理出来。</p>
<p><strong>代码2.2 - SparkContext中的组件字段及Getter</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _conf: <span class="type">SparkConf</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _listenerBus: <span class="type">LiveListenerBus</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _env: <span class="type">SparkEnv</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _statusTracker: <span class="type">SparkStatusTracker</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _progressBar: <span class="type">Option</span>[<span class="type">ConsoleProgressBar</span>] = <span class="type">None</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _ui: <span class="type">Option</span>[<span class="type">SparkUI</span>] = <span class="type">None</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _hadoopConfiguration: <span class="type">Configuration</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: <span class="type">SchedulerBackend</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _taskScheduler: <span class="type">TaskScheduler</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _heartbeatReceiver: <span class="type">RpcEndpointRef</span> = _</span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> _dagScheduler: <span class="type">DAGScheduler</span> = _</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _eventLogger: <span class="type">Option</span>[<span class="type">EventLoggingListener</span>] = <span class="type">None</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _executorAllocationManager: <span class="type">Option</span>[<span class="type">ExecutorAllocationManager</span>] = <span class="type">None</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _cleaner: <span class="type">Option</span>[<span class="type">ContextCleaner</span>] = <span class="type">None</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _statusStore: <span class="type">AppStatusStore</span> = _</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">conf</span></span>: <span class="type">SparkConf</span> = _conf</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">listenerBus</span></span>: <span class="type">LiveListenerBus</span> = _listenerBus</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">env</span></span>: <span class="type">SparkEnv</span> = _env</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">statusTracker</span></span>: <span class="type">SparkStatusTracker</span> = _statusTracker</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">progressBar</span></span>: <span class="type">Option</span>[<span class="type">ConsoleProgressBar</span>] = _progressBar</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">ui</span></span>: <span class="type">Option</span>[<span class="type">SparkUI</span>] = _ui</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hadoopConfiguration</span></span>: <span class="type">Configuration</span> = _hadoopConfiguration</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">schedulerBackend</span></span>: <span class="type">SchedulerBackend</span> = _schedulerBackend</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">taskScheduler</span></span>: <span class="type">TaskScheduler</span> = _taskScheduler</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">taskScheduler_=</span></span>(ts: <span class="type">TaskScheduler</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _taskScheduler = ts</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">dagScheduler</span></span>: <span class="type">DAGScheduler</span> = _dagScheduler</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">dagScheduler_=</span></span>(ds: <span class="type">DAGScheduler</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _dagScheduler = ds</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">eventLogger</span></span>: <span class="type">Option</span>[<span class="type">EventLoggingListener</span>] = _eventLogger</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">executorAllocationManager</span></span>: <span class="type">Option</span>[<span class="type">ExecutorAllocationManager</span>] = _executorAllocationManager</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">cleaner</span></span>: <span class="type">Option</span>[<span class="type">ContextCleaner</span>] = _cleaner</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">statusStore</span></span>: <span class="type">AppStatusStore</span> = _statusStore</span><br></pre></td></tr></table></div></figure>

<p>下面，我们按照SparkContext初始化的实际顺序，依次对这些组件作简要的了解，并且会附上一部分源码。</p>

        <h4 id="SparkConf"   >
          <a href="#SparkConf" class="heading-link"><i class="fas fa-link"></i></a>SparkConf</h4>
      <p>SparkConf在上一篇中已经详细讲过。它其实不算初始化的组件，因为它是构造SparkContext时传进来的参数。SparkContext会先将传入的SparkConf克隆一份副本，之后在副本上做校验（主要是应用名和Master的校验），及添加其他必须的参数（Driver地址、应用ID等）。这样用户就不可以再更改配置项，以保证Spark配置在运行期的不变性。</p>

        <h4 id="LiveListenerBus"   >
          <a href="#LiveListenerBus" class="heading-link"><i class="fas fa-link"></i></a>LiveListenerBus</h4>
      <p>LiveListenerBus是SparkContext中的事件总线。它异步地将事件源产生的事件（SparkListenerEvent）投递给已注册的监听器（SparkListener）。Spark中广泛运用了监听器模式，以适应集群状态下的分布式事件汇报。</p>
<p>除了它之外，Spark中还有多种事件总线，它们都继承自ListenerBus特征。事件总线是Spark底层的重要支撑组件，之后会专门分析。</p>

        <h4 id="AppStatusStore"   >
          <a href="#AppStatusStore" class="heading-link"><i class="fas fa-link"></i></a>AppStatusStore</h4>
      <p>AppStatusStore提供Spark程序运行中各项监控指标的键值对化存储。Web UI中见到的数据指标基本都存储在这里。其初始化代码如下。</p>
<p><strong>代码2.3 - 构造方法中AppStatusStore的初始化</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">_statusStore = <span class="type">AppStatusStore</span>.createLiveStore(conf)</span><br><span class="line">listenerBus.addToStatusQueue(_statusStore.listener.get)</span><br></pre></td></tr></table></div></figure>

<p><strong>代码2.4 - o.a.s.status.AppStatusStore.createLiveStore()方法</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createLiveStore</span></span>(conf: <span class="type">SparkConf</span>): <span class="type">AppStatusStore</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> store = <span class="keyword">new</span> <span class="type">ElementTrackingStore</span>(<span class="keyword">new</span> <span class="type">InMemoryStore</span>(), conf)</span><br><span class="line">    <span class="keyword">val</span> listener = <span class="keyword">new</span> <span class="type">AppStatusListener</span>(store, conf, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">new</span> <span class="type">AppStatusStore</span>(store, listener = <span class="type">Some</span>(listener))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></div></figure>



<p>可见，AppStatusStore底层使用了ElementTrackingStore，它是能够跟踪元素及其数量的键值对存储结构，因此适合用于监控。</p>
<p>另外还会产生一个监听器AppStatusListener的实例，并注册到前述LiveListenerBus中，用来收集监控数据。</p>

        <h4 id="SparkEnv"   >
          <a href="#SparkEnv" class="heading-link"><i class="fas fa-link"></i></a>SparkEnv</h4>
      <p>SparkEnv是Spark中的执行环境。</p>
<p>Driver与Executor的运行都需要SparkEnv提供的各类组件形成的环境来作为基础。其初始化代码如下。</p>
<p><strong>代码2.5 - 构造方法中SparkEnv的初始化</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">_env = createSparkEnv(_conf, isLocal, listenerBus)</span><br><span class="line"><span class="type">SparkEnv</span>.set(_env)</span><br></pre></td></tr></table></div></figure>

<p><strong>代码2.6 - o.a.s.SparkContext.createSparkEnv()方法</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createSparkEnv</span></span>(</span><br><span class="line">conf: <span class="type">SparkConf</span>,</span><br><span class="line">isLocal: <span class="type">Boolean</span>,</span><br><span class="line">listenerBus: <span class="type">LiveListenerBus</span>): <span class="type">SparkEnv</span> = &#123;</span><br><span class="line"><span class="type">SparkEnv</span>.createDriverEnv(conf, isLocal, listenerBus, <span class="type">SparkContext</span>.numDriverCores(master))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>可见，SparkEnv的初始化依赖于LiveListenerBus，并且在SparkContext初始化时只会创建Driver的执行环境，Executor的执行环境就是后话了。在创建Driver执行环境后，会调用SparkEnv伴生对象中的set()方法保存它，这样就可以“一处创建，多处使用”SparkEnv。</p>
<p>通过SparkEnv管理的组件也有多种，比如SparkContext中就会出现的安全性管理器SecurityManager、RPC环境RpcEnv、存储块管理器BlockManager、监控度量系统MetricsSystem。在SparkContext构造方法的后方，就会藉由SparkEnv先初始化BlockManager与启动MetricsSystem。</p>
<p><strong>代码2.7 - 构造方法中BlockManager的初始化与MetricsSystem的启动</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.blockManager.initialize(_applicationId)</span><br><span class="line">_env.metricsSystem.start()</span><br><span class="line">_env.metricsSystem.getServletHandlers.foreach(handler =&gt; ui.foreach(_.attachHandler(handler)))</span><br></pre></td></tr></table></div></figure>

<p>由于SparkEnv的重要性和复杂性，后面会专门写文章来讲解它，这里只需要有个大致的了解即可。</p>

        <h4 id="SparkStatusTracker"   >
          <a href="#SparkStatusTracker" class="heading-link"><i class="fas fa-link"></i></a>SparkStatusTracker</h4>
      <p>SparkStatusTracker提供报告最近作业执行情况的低级API。它的内部只有6个方法，从AppStatusStore中查询并返回诸如Job/Stage ID、活跃/完成/失败的Task数、Executor内存用量等基础数据。它只能保证非常弱的一致性语义，也就是说它报告的信息会有延迟或缺漏。</p>

        <h4 id="ConsoleProgressBar"   >
          <a href="#ConsoleProgressBar" class="heading-link"><i class="fas fa-link"></i></a>ConsoleProgressBar</h4>
      <p>ConsoleProgressBar按行打印Stage的计算进度。它周期性地从AppStatusStore中查询Stage对应的各状态的Task数，并格式化成字符串输出。它可以通过spark.ui.showConsoleProgress参数控制开关，默认值false。</p>

        <h4 id="SparkUI"   >
          <a href="#SparkUI" class="heading-link"><i class="fas fa-link"></i></a>SparkUI</h4>
      <p>SparkUI维护监控数据在Spark Web UI界面的展示。它的样子在文章#0的图中已经出现过，因此不再赘述。其初始化代码如下。</p>
<p><strong>代码2.8 - 构造方法中SparkUI的初始化</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">_ui =</span><br><span class="line"><span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.ui.enabled"</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">        <span class="type">Some</span>(<span class="type">SparkUI</span>.create(<span class="type">Some</span>(<span class="keyword">this</span>), _statusStore, _conf, _env.securityManager, appName, <span class="string">""</span>,</span><br><span class="line">          startTime))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    _ui.foreach(_.bind())</span><br></pre></td></tr></table></div></figure>

<p>可以通过spark.ui.enabled参数来控制是否启用Spark UI，默认值true。然后调用SparkUI的父类WebUI的<code>bind()</code>方法，将Spark UI绑定到特定的host:port上，如前面文章中的localhost:4040。</p>

        <h4 id="Hadoop-Configuration"   >
          <a href="#Hadoop-Configuration" class="heading-link"><i class="fas fa-link"></i></a>(Hadoop)Configuration</h4>
      <p>Spark可能会依赖于Hadoop的一些组件运行，如HDFS和YARN，Spark官方也有针对Hadoop 2.6/2.7的预编译包供下载。</p>
<p>SparkContext会借助工具类SparkHadoopUtil初始化一些与Hadoop有关的配置，存放在Hadoop的Configuration实例中，如Amazon S3相关的配置，和以“spark.hadoop.”为前缀的Spark配置参数。</p>

        <h4 id="HeartbeatReceiver"   >
          <a href="#HeartbeatReceiver" class="heading-link"><i class="fas fa-link"></i></a>HeartbeatReceiver</h4>
      <p>HeartbeatReceiver是心跳接收器。Executor需要向Driver定期发送心跳包来表示自己存活。它本质上也是个监听器，继承了SparkListener。其初始化代码如下。</p>
<p><strong>代码2.9 - 构造方法中HeartbeatReceiver的初始化</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_heartbeatReceiver = env.rpcEnv.setupEndpoint(<span class="type">HeartbeatReceiver</span>.<span class="type">ENDPOINT_NAME</span>, <span class="keyword">new</span> <span class="type">HeartbeatReceiver</span>(<span class="keyword">this</span>))</span><br></pre></td></tr></table></div></figure>

<p>可见，HeartbeatReceiver通过RpcEnv最终包装成了一个RPC端点的引用，即代码2.2中的RpcEndpointRef。</p>
<p>Spark集群的节点间必然会涉及大量的网络通信，心跳机制只是其中的一方面而已。因此RPC框架同事件总线一样，是Spark底层不可或缺的组成部分。</p>

        <h4 id="SchedulerBackend"   >
          <a href="#SchedulerBackend" class="heading-link"><i class="fas fa-link"></i></a>SchedulerBackend</h4>
      <p>SchedulerBackend负责向等待计算的Task分配计算资源，并在Executor上启动Task。它是一个Scala特征，有多种部署模式下的SchedulerBackend实现类。它在SparkContext中是和TaskScheduler一起初始化的，作为一个元组返回。</p>

        <h4 id="TaskScheduler"   >
          <a href="#TaskScheduler" class="heading-link"><i class="fas fa-link"></i></a>TaskScheduler</h4>
      <p>TaskScheduler即任务调度器。它也是一个Scala特征，但只有一种实现，即TaskSchedulerImpl类。它负责提供Task的调度算法，并且会持有SchedulerBackend的实例，通过SchedulerBackend发挥作用。它们两个的初始化代码如下。</p>
<p><strong>代码2.10 - 构造方法中SchedulerBackend与TaskScheduler的初始化</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (sched, ts) = <span class="type">SparkContext</span>.createTaskScheduler(<span class="keyword">this</span>, master, deployMode)</span><br><span class="line">_schedulerBackend = sched</span><br><span class="line">_taskScheduler = ts</span><br></pre></td></tr></table></div></figure>

<p>下面这个方法比较长，但就当做是提前知道一下Spark的几种Master设置方式吧。</p>
<p>它包括有三种</p>
<p>本地模式、本地集群模式、Standalone模式，以及第三方集群管理器（如YARN）提供的模式。</p>
<p><strong>代码2.11 - o.a.s.SparkContext.createTaskScheduler()方法</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createTaskScheduler</span></span>(</span><br><span class="line">      sc: <span class="type">SparkContext</span>,</span><br><span class="line">      master: <span class="type">String</span>,</span><br><span class="line">      deployMode: <span class="type">String</span>): (<span class="type">SchedulerBackend</span>, <span class="type">TaskScheduler</span>) = &#123;</span><br><span class="line"><span class="keyword">import</span> <span class="type">SparkMasterRegex</span>._</span><br><span class="line"></span><br><span class="line"><span class="comment">// When running locally, don't try to re-execute tasks on failure.</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">MAX_LOCAL_TASK_FAILURES</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    master <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="string">"local"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)</span><br><span class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, <span class="number">1</span>)</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="type">LOCAL_N_REGEX</span>(threads) =&gt;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()</span><br><span class="line"><span class="comment">// local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.</span></span><br><span class="line">        <span class="keyword">val</span> threadCount = <span class="keyword">if</span> (threads == <span class="string">"*"</span>) localCpuCount <span class="keyword">else</span> threads.toInt</span><br><span class="line"><span class="keyword">if</span> (threadCount &lt;= <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Asked to run locally with <span class="subst">$threadCount</span> threads"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)</span><br><span class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, threadCount)</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="type">LOCAL_N_FAILURES_REGEX</span>(threads, maxFailures) =&gt;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()</span><br><span class="line"><span class="comment">// local[*, M] means the number of cores on the computer with M failures</span></span><br><span class="line"><span class="comment">// local[N, M] means exactly N threads with M failures</span></span><br><span class="line">        <span class="keyword">val</span> threadCount = <span class="keyword">if</span> (threads == <span class="string">"*"</span>) localCpuCount <span class="keyword">else</span> threads.toInt</span><br><span class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, maxFailures.toInt, isLocal = <span class="literal">true</span>)</span><br><span class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, threadCount)</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="type">SPARK_REGEX</span>(sparkUrl) =&gt;</span><br><span class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc)</span><br><span class="line">        <span class="keyword">val</span> masterUrls = sparkUrl.split(<span class="string">","</span>).map(<span class="string">"spark://"</span> + _)</span><br><span class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">StandaloneSchedulerBackend</span>(scheduler, sc, masterUrls)</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="type">LOCAL_CLUSTER_REGEX</span>(numSlaves, coresPerSlave, memoryPerSlave) =&gt;</span><br><span class="line"><span class="comment">// Check to make sure memory requested &lt;= memoryPerSlave. Otherwise Spark will just hang.</span></span><br><span class="line">        <span class="keyword">val</span> memoryPerSlaveInt = memoryPerSlave.toInt</span><br><span class="line"><span class="keyword">if</span> (sc.executorMemory &gt; memoryPerSlaveInt) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line"><span class="string">"Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker"</span>.format(</span><br><span class="line">              memoryPerSlaveInt, sc.executorMemory))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc)</span><br><span class="line">        <span class="keyword">val</span> localCluster = <span class="keyword">new</span> <span class="type">LocalSparkCluster</span>(</span><br><span class="line">          numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)</span><br><span class="line">        <span class="keyword">val</span> masterUrls = localCluster.start()</span><br><span class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">StandaloneSchedulerBackend</span>(scheduler, sc, masterUrls)</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        backend.shutdownCallback = (backend: <span class="type">StandaloneSchedulerBackend</span>) =&gt; &#123;</span><br><span class="line">          localCluster.stop()</span><br><span class="line">        &#125;</span><br><span class="line">        (backend, scheduler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> masterUrl =&gt;</span><br><span class="line">        <span class="keyword">val</span> cm = getClusterManager(masterUrl) <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Some</span>(clusterMgr) =&gt; clusterMgr</span><br><span class="line"><span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Could not parse Master URL: '"</span> + master + <span class="string">"'"</span>)</span><br><span class="line">        &#125;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> scheduler = cm.createTaskScheduler(sc, masterUrl)</span><br><span class="line">          <span class="keyword">val</span> backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)</span><br><span class="line">          cm.initialize(scheduler, backend)</span><br><span class="line">          (backend, scheduler)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line"><span class="keyword">case</span> se: <span class="type">SparkException</span> =&gt; <span class="keyword">throw</span> se</span><br><span class="line"><span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"External scheduler cannot be instantiated"</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></div></figure>


        <h4 id="DAGScheduler"   >
          <a href="#DAGScheduler" class="heading-link"><i class="fas fa-link"></i></a>DAGScheduler</h4>
      <p>DAGScheduler即有向无环图（DAG）调度器。</p>
<p>DAG的概念在前面文章中已经讲过，用来表示RDD之间的血缘。</p>
<p>DAGScheduler负责生成并提交Job，以及按照DAG将RDD和算子划分并提交Stage。</p>
<p>每个Stage都包含一组Task，称为TaskSet，它们被传递给TaskScheduler。</p>
<p>也就是说DAGScheduler需要先于TaskScheduler进行调度。</p>
<p>DAGScheduler初始化是直接new出来的，但在其构造方法里也会将SparkContext中TaskScheduler的引用传进去。因此要等DAGScheduler创建后，再真正启动TaskScheduler。</p>
<p><strong>代码2.12 - 构造方法中DAGScheduler的初始化和TaskScheduler的启动</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">_dagScheduler = <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>)</span><br><span class="line">_heartbeatReceiver.ask[<span class="type">Boolean</span>](<span class="type">TaskSchedulerIsSet</span>)</span><br><span class="line">_taskScheduler.start()</span><br></pre></td></tr></table></div></figure>

<p>SchedulerBackend、TaskScheduler与DAGScheduler是Spark调度逻辑的主要组成部分，之后会深入探索它们的细节。</p>
<p>值得注意的是，代码2.2中只有TaskScheduler与DAGScheduler还定义了Setter方法，目前只在内部测试方法中调用过。</p>

        <h4 id="EventLoggingListener"   >
          <a href="#EventLoggingListener" class="heading-link"><i class="fas fa-link"></i></a>EventLoggingListener</h4>
      <p>EventLoggingListener是用于事件持久化的监听器。它可以通过spark.eventLog.enabled参数控制开关，默认值false。如果开启，它也会注册到LiveListenerBus里，并将特定的一部分事件写到磁盘。</p>

        <h4 id="ExecutorAllocationManager"   >
          <a href="#ExecutorAllocationManager" class="heading-link"><i class="fas fa-link"></i></a>ExecutorAllocationManager</h4>
      <p>ExecutorAllocationManager即Executor分配管理器。</p>
<p>它可以通过spark.dynamicAllocation.enabled参数控制开关，默认值false。</p>
<p>如果开启，并且SchedulerBackend的实现类支持这种机制，Spark就会根据程序运行时的负载动态增减Executor的数量。</p>
<p>它的初始化代码如下。</p>
<p><strong>代码2.13 - 构造方法中ExecutorAllocationManager的初始化</strong></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dynamicAllocationEnabled = <span class="type">Utils</span>.isDynamicAllocationEnabled(_conf)</span><br><span class="line">    _executorAllocationManager =</span><br><span class="line"><span class="keyword">if</span> (dynamicAllocationEnabled) &#123;</span><br><span class="line">        schedulerBackend <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> b: <span class="type">ExecutorAllocationClient</span> =&gt;</span><br><span class="line">                    <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ExecutorAllocationManager</span>(</span><br><span class="line">                      schedulerBackend.asInstanceOf[<span class="type">ExecutorAllocationClient</span>], listenerBus, _conf,</span><br><span class="line">                      _env.blockManager.master))</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">                    <span class="type">None</span></span><br><span class="line">                &#125;</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">None</span></span><br><span class="line">              &#125;</span><br><span class="line">            _executorAllocationManager.foreach(_.start())</span><br></pre></td></tr></table></div></figure>


        <h4 id="ContextCleaner"   >
          <a href="#ContextCleaner" class="heading-link"><i class="fas fa-link"></i></a>ContextCleaner</h4>
      <p>ContextCleaner即上下文清理器。它可以通过spark.cleaner.referenceTracking参数控制开关，默认值true。它内部维护着对RDD、Shuffle依赖和广播变量（之后会提到）的弱引用，如果弱引用的对象超出程序的作用域，就异步地将它们清理掉。</p>

        <h3 id="总结"   >
          <a href="#总结" class="heading-link"><i class="fas fa-link"></i></a>总结</h3>
      <p>本文从SparkContext的构造方法入手，按顺序简述了十余个Spark内部组件及其初始化逻辑。这些组件覆盖了Spark机制的多个方面，我们之后在适当的时机还要深入研究其中的一部分，特别重要的如事件总线LiveListenerBus、执行环境SparkEnv、调度器TaskScheduler及DAGScheduler等。</p>
<p>最后用一张图来概括吧。</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://lormer.cn/sq2uE6cicHYyGduHdniaiczqQs6dj6WOdts4nzicrvFQLqCe9ET5ml3gWBMjP7s4kiaxq8AKjM5U5uwVHMNN5VmAFgg.jpg"  alt="img">
      </p>
<p>除了组件初始化之外，SparkContext内还有其他一部分有用的属性。并且在初始化的主流程做完之后，也有不少善后工作要做。下一篇文章就会分析它们。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://chenhefei.github.io">Paradiser</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://chenhefei.github.io/2020/06/04/Spark/SparkCore_src_02_SparkContext-Component-initialization/">https://chenhefei.github.io/2020/06/04/Spark/SparkCore_src_02_SparkContext-Component-initialization/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://chenhefei.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://chenhefei.github.io/tags/Spark/">Spark</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2020/06/04/Spark/SparkCore_src_03_SparkContext-Secondary-properties-and-post-initialization/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Spark Core - 源码分析_03 - SparkContext辅助属性及后初始化</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2020/06/04/Spark/SparkCore_src_01_SparkConf/"><span class="paginator-prev__text">Spark Core - 源码分析_01 - SparkConf</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Core源码精读-02-SparkContext组件初始化"><span class="toc-text">
          Spark Core源码精读_02 | SparkContext组件初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#前言"><span class="toc-text">
          前言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkContext类的构造方法"><span class="toc-text">
          SparkContext类的构造方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkContext初始化的组件"><span class="toc-text">
          SparkContext初始化的组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SparkConf"><span class="toc-text">
          SparkConf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LiveListenerBus"><span class="toc-text">
          LiveListenerBus</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AppStatusStore"><span class="toc-text">
          AppStatusStore</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SparkEnv"><span class="toc-text">
          SparkEnv</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SparkStatusTracker"><span class="toc-text">
          SparkStatusTracker</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ConsoleProgressBar"><span class="toc-text">
          ConsoleProgressBar</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SparkUI"><span class="toc-text">
          SparkUI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-Configuration"><span class="toc-text">
          (Hadoop)Configuration</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HeartbeatReceiver"><span class="toc-text">
          HeartbeatReceiver</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SchedulerBackend"><span class="toc-text">
          SchedulerBackend</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TaskScheduler"><span class="toc-text">
          TaskScheduler</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DAGScheduler"><span class="toc-text">
          DAGScheduler</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EventLoggingListener"><span class="toc-text">
          EventLoggingListener</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ExecutorAllocationManager"><span class="toc-text">
          ExecutorAllocationManager</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ContextCleaner"><span class="toc-text">
          ContextCleaner</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-text">
          总结</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">纸短情长</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/chenhefei/" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">12</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">10</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">9</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2020</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Paradiser</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v4.2.1</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.0.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (false) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"></div><script src="/js/utils.js?v=2.0.1"></script><script src="/js/stun-boot.js?v=2.0.1"></script><script src="/js/scroll.js?v=2.0.1"></script><script src="/js/header.js?v=2.0.1"></script><script src="/js/sidebar.js?v=2.0.1"></script></body></html>